import Image from "next/image";
import WebNn from "../../../../../app/_components/icons/webnn.jsx";
import VIcon from "../../../../../app/_components/icons/v.jsx";
import InfoIcon from '../../../../../app/_components/icons/info.jsx'
import { Cards } from 'nextra/components'

<div className="mt-6 grid grid-cols-1 items-center justify-items-center tutorials-title">
  <WebNn className="webnn-logo"/>
</div>

# Generating WebNN Vanilla JavaScript

Vanilla JavaScript offers the smallest bundle size for WebNN applications, making it ideal for package-size sensitive deployments. However, manually writing WebNN code requires significant development effort compared to higher-level frameworks like [Transformers.js](https://huggingface.co/docs/transformers.js/en/index) or [ONNX Runtime Web](https://onnxruntime.ai/).

The WebNN community addresses this challenge by providing code generation tools that automatically convert existing machine learning models into optimized WebNN JavaScript, delivering both performance and minimal bundle overhead.

## Bundle Size vs Development Efficiency

When building web-based ML applications, developers face a key trade-off:

- **High-level libraries** ([Transformers.js](https://huggingface.co/docs/transformers.js/en/index), [ONNX Runtime Web](https://onnxruntime.ai/)) offer rapid development but larger bundle sizes
- **Vanilla WebNN JavaScript** provides minimal footprint but requires extensive manual coding
- **Generated WebNN code** combines the best of both: automated development with optimized bundle size

## Model-to-Code Conversion

Currently, the WebNN ecosystem provides three primary code generation approaches, these tools convert trained ML models directly into WebNN lean vanilla JavaScript:

<Cards className="quick-start-card">
  <Cards.Card
    icon={<VIcon />}
    title="ONNX2WebNN"
    href="./onnx2webnn"
  />
  <Cards.Card
    icon={<VIcon />}
    title="WebNN Code Generator"  
    href="./webnn-code-generator"
  />
  <Cards.Card
    icon={<VIcon />}
    title="WebNNUtils Onnx Converter"
    href="./webnnutils-onnxconverter"
  />
</Cards>

| Feature | ONNX2WebNN | WebNN Code Generator | WebNNUtils/OnnxConverter |
|---------|:----------:|:-------------------:|:------------------------:|
| **Interface** | Command-line (CLI) | Web application | Python toolkit |
| **Language** | Python | TypeScript | Python |
| **ONNX Support** | âœ… Yes | âœ… Preferred | âœ… Yes |
| **TensorFlow Lite Support** | âŒ Not supported | ðŸ”§ Yes with compatibility issues | âŒ Not supported |
| **Other Formats Support** | âŒ Not supported | ðŸ”§ Yes with compatibility issues | âŒ Not supported |
| **Privacy** | Client-side only | Client-side only | Client-side only |

### [ONNX2WebNN](https://github.com/huningxin/onnx2webnn)
- **Best for**: Automated workflows, CI/CD pipelines, batch processing
- **Strengths**: Mature Python ecosystem, command-line automation
- **Use case**: Converting both individual and multiple models in development pipelines

### [WebNN Code Generator](https://ibelem.github.io/webnn-code-generator/) 
- **Best for**: Interactive development, quick prototyping
- **Strengths**: No installation required, browser-based privacy protection
- **Use case**: Converting individual models with complete data privacy

### [WebNNUtils/OnnxConverter](https://github.com/MicrosoftEdge/WebNNUtils/tree/main/OnnxConverter)
- **Best for**: Production deployments, complex model optimization
- **Strengths**: Advanced graph partitioning, model reordering, Microsoft Edge team support
- **Use case**: Enterprise applications requiring optimized WebNN deployment with CPU/NPU acceleration

## Code-to-Code Translation

Converting existing ML code from other frameworks to WebNN lean vanilla JavaScript:

### Python to WebNN vanilla JavaScript

**Current Status**: ðŸš§ **To do**

Translate Python-based ML code (particularly PyTorch/TorchScript) directly to WebNN JavaScript is also required for WebNN developers. This approach would enable:

- TorchScript â†’ WebNN JavaScript conversion
- Custom operator mapping, translate complex Python ML logic to WebNN
- Automatic optimization for web deployment

> <InfoIcon /> This functionality represents a significant development effort and is not yet available. The gap between Python ML frameworks and WebNN JavaScript remains an active area of community development.

## Getting Started

1. **Evaluate your requirements**: Bundle size constraints, privacy needs, development workflow, performance requirements
2. **Choose your tool**: CLI automation (ONNX2WebNN), advanced optimization (WebNNUtils/OnnxConverter), or browser interface (Code Generator)
3. **Prepare your model**: Ensure ONNX compatibility and optimization for web deployment
4. **Generate and test**: Convert your model and validate performance in target environments
5. **Deploy and monitor**: Integrate generated code and monitor real-world performance

The WebNN code generation ecosystem provides flexible options for deploying optimized ML inference in web applications while maintaining the performance benefits of vanilla JavaScript.